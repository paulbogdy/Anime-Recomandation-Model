{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To be used on Google Colaboratory for TPU accelerator\n",
    "%load_ext tensorboard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "# If used on google colab, run on TPU\n",
    "TPU_INIT = True\n",
    "\n",
    "if TPU_INIT:\n",
    "  try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "  except ValueError:\n",
    "    raise BaseException('ERROR: Not connected to a TPU runtime!')\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "  !nvidia-smi\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the anime and ratings from the preprocessed data\n",
    "anime = pd.read_csv(\"/content/drive/MyDrive/anime.csv\")\n",
    "ratings = pd.read_csv(\"/content/drive/MyDrive/ratings.csv\")\n",
    "\n",
    "print(f\"There are {len(ratings)} ratings\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now we need to create an encoding in order to use the Embedding layer\n",
    "\n",
    "user_ids = ratings['user_id'].unique().tolist()\n",
    "user2encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "encoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "\n",
    "anime_ids = ratings['anime_id'].unique().tolist()\n",
    "anime2encoded = {x: i for i, x in enumerate(anime_ids)}\n",
    "encoded2anime = {i: x for i, x in enumerate(anime_ids)}\n",
    "\n",
    "ratings['user'] = ratings['user_id'].map(user2encoded)\n",
    "ratings['anime'] = ratings['anime_id'].map(anime2encoded)\n",
    "\n",
    "num_users = len(user_ids)\n",
    "num_anime = len(anime_ids)\n",
    "\n",
    "max_rating = ratings['rating'].max()\n",
    "min_rating = ratings['rating'].min()\n",
    "\n",
    "print(f\"{max_rating} max rating, {min_rating} min rating, {num_users} users, {num_anime} anime\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# normalizing data and splitting into train/validation\n",
    "\n",
    "ratings = ratings.sample(frac=1, random_state=69)\n",
    "x = ratings[['user', 'anime']].values\n",
    "y = ratings['rating'].apply(lambda x: (x-min_rating)/(max_rating-min_rating)).values\n",
    "train_indices = int(0.9 * ratings.shape[0])\n",
    "\n",
    "x_train, x_val, y_train, y_val = (\n",
    "    x[:train_indices],\n",
    "    x[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping for the training phase, when we just update parameters\n",
    "# I chose patience 2, because the model converges kinda fast\n",
    "early_stopping = EarlyStopping(patience = 2, monitor='val_loss',\n",
    "                               mode='min', restore_best_weights=True)\n",
    "\n",
    "batch_size = 8192\n",
    "start_lr = 0.00001\n",
    "min_lr = 0.00001\n",
    "max_lr = 0.00005\n",
    "\n",
    "rampup_epochs = 2\n",
    "sustain_epochs = 0\n",
    "exp_decay = .8\n",
    "\n",
    "if TPU_INIT:\n",
    "  max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n",
    "  batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# Learning rate scheduler for better training peace\n",
    "# You are free to play with the parameters for this, for me those worked the best\n",
    "def lrfn(epoch):\n",
    "  if epoch < rampup_epochs:\n",
    "    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
    "  elif epoch < rampup_epochs + sustain_epochs:\n",
    "    return max_lr\n",
    "  else:\n",
    "    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
    "\n",
    "my_callbacks = [\n",
    "    lr_callback,\n",
    "    early_stopping,\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Dot, Add, Activation, BatchNormalization, Flatten, Input, Dense\n",
    "\n",
    "def RecommenderNet():\n",
    "    embedding_size = 32\n",
    "\n",
    "    user = Input(name = 'user', shape = [1])\n",
    "    user_embedding = Embedding(name = 'user_embedding',\n",
    "                       input_dim = num_users,\n",
    "                       output_dim = embedding_size,\n",
    "                       embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6))(user)\n",
    "    user_bias = Embedding(name = 'user_bias',\n",
    "                          input_dim = num_users,\n",
    "                          output_dim = 1)(user)\n",
    "\n",
    "    anime = Input(name = 'anime', shape = [1])\n",
    "    anime_embedding = Embedding(name = 'anime_embedding',\n",
    "                       input_dim = num_anime,\n",
    "                       output_dim = embedding_size,\n",
    "                       embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6))(anime)\n",
    "    anime_bias = Embedding(name = 'anime_bias',\n",
    "                           input_dim = num_anime,\n",
    "                           output_dim = 1)(anime)\n",
    "\n",
    "    x = Dot(name = 'dot_product', normalize = True, axes = 2)([user_embedding, anime_embedding])\n",
    "    x = Add(name = 'added')([x, anime_bias, user_bias])\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(1, kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[user, anime], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', metrics=[\"mae\", \"mse\"], optimizer='Adam')\n",
    "    return model\n",
    "if TPU_INIT:\n",
    "  with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
    "    model = RecommenderNet()\n",
    "else:\n",
    "  model = RecommenderNet()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=[x_train[:,0], x_train[:, 1]],\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=([x_val[:,0], x_val[:, 1]], y_val),\n",
    "    callbacks=my_callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper right\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anime_weights = model.get_weights()[1]\n",
    "anime['MAL_ID'] = anime['MAL_ID'].apply(lambda x: anime2encoded[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The Euclidean distance between two points in Euclidean space.\n",
    "    # Arguments\n",
    "        y_true: tensor with true targets.\n",
    "        y_pred: tensor with predicted targets.\n",
    "    # Returns\n",
    "        float type Euclidean distance between two data points.\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def angle(y_true, y_pred):\n",
    "  return tf.keras.losses.cosine_similarity(y_true, y_pred)\n",
    "\n",
    "def find_anime(name):\n",
    "    return anime.loc[anime['Name'] == name]['MAL_ID'].values[0]\n",
    "\n",
    "def similar(anime_list):\n",
    "    in_list = []\n",
    "    for anime_name in anime_list:\n",
    "        anime_id = find_anime(anime_name)\n",
    "        in_list.append(anime_id)\n",
    "    distance = np.array([0]*num_anime)\n",
    "    for anime_id in in_list:\n",
    "        distance += angle(anime_weights, anime_weights[anime_id])\n",
    "    top10 = pd.Series(distance).sort_values().drop(labels=in_list).head(10)\n",
    "    print(top10)\n",
    "    series = top10.index.values\n",
    "    result = pd.DataFrame(columns = ['Name', 'Genders', 'Description'])\n",
    "    print(result)\n",
    "    for i, anime_id in enumerate(series):\n",
    "        row = anime.loc[anime['MAL_ID'] == anime_id]\n",
    "        new_row = {\"Name\": row['Name'].values[0],\n",
    "                  \"Genders\": row['Genres'].values[0],\n",
    "                  \"Description\": row['Description'].values[0]}\n",
    "        result.loc[i] = new_row\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar(['Naruto'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}